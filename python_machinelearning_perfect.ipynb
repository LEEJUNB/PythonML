{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 中 문장 토큰화\n",
    "---\n",
    "### 문장 토큰화는 언제 쓰는가?\n",
    "* 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 떄 사용\n",
    "\n",
    "### 방법 1 : 일반적으로 문장의 마지막 기호 따라 분리\n",
    "    - NLTK에서 가장 일반적으로 쓰이는 sent_tokenize\n",
    "        - sent_tokenize가 반환하는 것은 각각 문장으로 구성된 list 객체\n",
    "    \n",
    "### 방법 2 : 정규 표현식에 따른 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JBE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # 마침표, 개행 문자 등의 데이터셋 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. you can see it out your window or on your tv'\n",
    "sentences = sent_tokenize(text=text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'you can see it out your window or on your tv']\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 中 단어 토큰화\n",
    "---\n",
    "### 단어 토큰화는 언제쓰는가?\n",
    "- Bag of word 처럼 단어 순서가 중요하지 않은 경우 단어 토큰화 사용\n",
    "\n",
    "### 방법1 : 공백,콤마,마침표, 개행문자 등으로 분리\n",
    "### 방법2 : 정규표현식을 이용하여 다양한 유형으로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
