{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 中 문장 토큰화\n",
    "---\n",
    "### 문장 토큰화는 언제 쓰는가?\n",
    "* 각 문장이 가지는 시맨틱적인 의미가 중요한 요소로 사용될 떄 사용\n",
    "\n",
    "### 방법 1 : 일반적으로 문장의 마지막 기호 따라 분리\n",
    "    - NLTK에서 가장 일반적으로 쓰이는 sent_tokenize\n",
    "        - sent_tokenize가 반환하는 것은 각각 문장으로 구성된 list 객체\n",
    "    \n",
    "### 방법 2 : 정규 표현식에 따른 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\JBE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # 마침표, 개행 문자 등의 데이터셋 다운"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. you can see it out your window or on your tv'\n",
    "sentences = sent_tokenize(text=text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'you can see it out your window or on your tv']\n"
     ]
    }
   ],
   "source": [
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화 中 단어 토큰화\n",
    "---\n",
    "### 단어 토큰화는 언제쓰는가?\n",
    "- Bag of word 처럼 단어 순서가 중요하지 않은 경우 단어 토큰화 사용\n",
    "\n",
    "### 방법1 : 공백,콤마,마침표, 개행문자 등으로 분리\n",
    "### 방법2 : 정규표현식을 이용하여 다양한 유형으로 토큰화 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 단어 토큰화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 2\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['you', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'tv']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. you can see it out your window or on your tv'\n",
    "\n",
    "# 분리된 문장을 토큰화하는 함수\n",
    "def tokenize_text(text) :\n",
    "    # 먼저 문장별로 분리 토큰화\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 그 후 분리된 문장별로 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]  # Q) \n",
    "    return word_tokens\n",
    "\n",
    "# 문장별 단어 토큰화 수행\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens) # 2개의 문장을 문장별로 토큰화. 2개의 리스트 객체를 내포하는 리스트. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram\n",
    "* 문장을 단어 토큰화할 시 문맥적 의미가 무시되는 문제를 보완한 것.\n",
    "* 연속된 n개의 단어를 하나의 토큰화 단위로 분리함\n",
    "* n개 단어 크기 윈도우를 만들어 문장의 처음부터 오른쪽으로 움직이며 토큰화 수행\n",
    "---\n",
    "* 예시)\n",
    "* 문장 : \"Agent Smith knocks\"\n",
    "* 2-gram(bigram) 적용 : (Agent, Smith),(Smith,knocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [전처리] Stop Word 제거\n",
    "* 스톱워드 : 분석에 큰 의미 없는 단어, 문맥적으로 큰 의미없는 단어\n",
    "* 스톱워드 제거 이유? : 문법적 특성으로 빈번하게 등장하여 중요 단어로 인지될 우려있기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
